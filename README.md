# Knowledge Base Agent

A sophisticated personal knowledge management system that intelligently scrapes, processes, and synthesizes information from web sources and documents. The system provides a powerful chat-based interface with accordion navigation, full resource management, and chat history, querying a curated personal knowledge base using locally-run AI models.

## ğŸ—ï¸ Architecture

**Two-Part Solution:**
1. **Backend (Python/FastAPI)**: Web scraping (including PDF URLs), document processing, vector database, RAG implementation, and PostgreSQL for data persistence
2. **Frontend (Next.js 15)**: Modern accordion-based UI with clean navigation, chat interface, and resource management

## ğŸš€ Features

- âœ… **Accordion-Based UI**: Clean, collapsible navigation reducing visual clutter
- âœ… **PostgreSQL Integration**: Robust data storage with JSONB support for metadata
- âœ… **Real-time Chat**: Instant messaging with AI responses and source attribution
- âœ… **Source Attribution**: AI responses include relevant source citations with URLs
- âœ… **Resource Management**: Full CRUD operations - view, add, and delete knowledge sources
- âœ… **Mobile Responsive**: Works on desktop and mobile devices with wider sidebar (28-36rem)
- âœ… **Local AI**: Complete privacy with local LLM processing (Ollama llama3.1:8b)
- âœ… **Vector Search**: Semantic search with ChromaDB and nomic-embed-text
- âœ… **Chat Management**: Create, view, and delete chat sessions with CASCADE
- âœ… **Web Scraping**: Intelligent content extraction with Playwright + BeautifulSoup
- âœ… **PDF URL Scraping**: Direct scraping of PDF files from web URLs using PyPDF2
- âœ… **Document Upload**: Upload and process PDF, TXT, and EPUB files (up to 100MB)
- âœ… **Retry Logic**: Automatic retry for failed scrapes
- âœ… **Hover Interactions**: Subtle delete buttons with smooth opacity transitions

## âš¡ Quick Start

**Run everything with one command:**

```bash
# Start all services (PostgreSQL, Ollama, Backend, Frontend)
make dev
```

**Or use the startup script directly:**

```bash
./start-dev.sh
```

**Available commands:**
- `make dev` - Start all services
- `make stop` - Stop all services
- `make setup` - Initial setup and installation
- `make clean` - Clean up generated files

The script automatically:
- âœ… Checks system requirements
- âœ… Starts PostgreSQL and creates database
- âœ… Starts Ollama and downloads AI models
- âœ… Installs dependencies if needed
- âœ… Starts backend and frontend servers
- âœ… Provides helpful status messages

**Access points:**
- ğŸŒ **Frontend**: http://localhost:3000
- ğŸ”§ **Backend API**: http://localhost:8000
- ğŸ“š **API Docs**: http://localhost:8000/docs

Press `Ctrl+C` to stop all services.

## ğŸ“‹ Prerequisites

- **macOS** (optimized for M4 Pro)
- **Python 3.11**
- **Node.js 18+**
- **PostgreSQL 15**
- **Ollama** (for local AI models)

## ğŸ› ï¸ Installation

### Option 1: Automated Installation (Recommended)

```bash
# Clone or navigate to the project directory
cd /Users/ekip.kalir/Projects/Personal/knowledge-base-agent

# Run the automated installation script
./install.sh
```

### Option 2: Manual Installation

#### 1. Install System Dependencies

```bash
# Install essential development tools
brew install curl wget tree htop git

# Install PostgreSQL 15
brew install postgresql@15
brew services start postgresql@15

# Install Python 3.11
brew install python@3.11

# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 2. Database Setup

```bash
# Create database and user
createdb knowledge_base_agent
psql knowledge_base_agent -c "CREATE USER kba_user WITH PASSWORD 'secure_password';"
psql knowledge_base_agent -c "GRANT ALL PRIVILEGES ON DATABASE knowledge_base_agent TO kba_user;"

# Create tables
psql knowledge_base_agent -c "
CREATE TABLE IF NOT EXISTS chat_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(255) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS chat_messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant')),
    sources JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS knowledge_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT UNIQUE NOT NULL,
    title VARCHAR(500),
    description TEXT,
    content TEXT,
    metadata JSONB,
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'error')),
    scraped_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_chat_messages_session_id ON chat_messages(session_id);
CREATE INDEX IF NOT EXISTS idx_chat_messages_created_at ON chat_messages(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_knowledge_sources_status ON knowledge_sources(status);
CREATE INDEX IF NOT EXISTS idx_knowledge_sources_url ON knowledge_sources(url);
"
```

#### 3. Backend Setup

```bash
cd knowledge-base-agent-backend

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
playwright install
```

#### 4. Frontend Setup

```bash
cd ../knowledge-base-agent-frontend

# Install dependencies
npm install
```

#### 5. Install AI Models

```bash
# Pull required Ollama models
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

## ğŸš€ Running the Application

### Option 1: Automated Startup (Recommended)

```bash
# Start all services with one command
./start-dev.sh
```

This will start:
- PostgreSQL (if not running)
- Ollama service
- Backend API server (port 8000)
- Frontend development server (port 3000)

### Option 2: Manual Startup

#### Terminal 1: Start Backend (Required)
```bash
# Quick start backend
./start-backend.sh

# OR manually:
cd knowledge-base-agent-backend
source venv/bin/activate
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Terminal 2: Start Frontend
```bash
cd knowledge-base-agent-frontend
npm run dev
```

#### Optional: Start Ollama (for AI features)
```bash
ollama serve
```

## ğŸŒ Access Points

- **Frontend Application**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Database**: PostgreSQL on localhost:5432

## ğŸ“š API Endpoints

### Chat Endpoints
- `POST /api/v1/chat/sessions` - Create new chat session
- `GET /api/v1/chat/sessions` - Get recent chat sessions (last 15)
- `GET /api/v1/chat/sessions/{id}/messages` - Get messages for a session
- `POST /api/v1/chat/sessions/{id}/messages` - Send message and get AI response with RAG
- `DELETE /api/v1/chat/sessions/{id}` - Delete chat session (CASCADE deletes messages)

### Knowledge Base Endpoints
- `POST /api/v1/scrape` - Scrape web content (including PDF URLs) and add to knowledge base
- `POST /api/v1/upload` - Upload documents (PDF, TXT, EPUB) to knowledge base
- `GET /api/v1/sources` - Get all knowledge sources with status
- `DELETE /api/v1/sources/{id}` - Delete knowledge source (CASCADE)
- `POST /api/v1/query` - Query knowledge base directly

## ğŸ”§ Configuration

### Backend Configuration (.env)
```env
# Application Settings
APP_NAME="Knowledge Base Agent Backend"
APP_VERSION="0.1.0"
DEBUG=true

# Database Settings
DATABASE_URL="postgresql://kba_user:secure_password@localhost:5432/knowledge_base_agent"

# Vector Database
VECTOR_DB_TYPE="chromadb"
CHROMA_DB_PATH="./data/chroma_db"

# Ollama Settings
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_CHAT_MODEL="llama3.1:8b"
OLLAMA_EMBED_MODEL="nomic-embed-text"

# API Settings
API_V1_STR="/api/v1"
CORS_ORIGINS=["http://localhost:3000"]
```

### Frontend Configuration (.env.local)
```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_APP_NAME="Knowledge Base Agent"
NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws
```

## ğŸ“– Usage Guide

### 1. First Time Setup
1. Start the application using `./start-dev.sh`
2. Navigate to http://localhost:3000
3. Click "New Chat" to start your first conversation

### 2. Adding Knowledge Sources

#### Using the Accordion UI
1. Open the "Add Web Source" accordion
2. Enter a URL (supports both HTML pages and PDF URLs like `https://arxiv.org/pdf/2510.06255`)
3. Click "Scrape URL"

#### Web Scraping (API)
```bash
# Scrape HTML page
curl -X POST "http://localhost:8000/api/v1/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/article"}'

# Scrape PDF URL directly
curl -X POST "http://localhost:8000/api/v1/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://arxiv.org/pdf/2510.06255"}'
```

#### Document Upload
```bash
# Upload a document via curl
curl -X POST "http://localhost:8000/api/v1/upload" \
  -F "file=@/path/to/your/document.pdf"

# Supported formats: PDF, TXT, EPUB
# Maximum file size: 100MB
```

#### Using the Web Interface
1. Navigate to http://localhost:3000
2. Open the "Upload Documents" accordion
3. Drag and drop files or click "Choose File" to browse
4. Supported formats: PDF, TXT, EPUB files

### 3. Managing Resources
- **View**: Open "Your Resources" accordion to see all indexed content
- **Delete**: Hover over any resource card and click the trash icon (becomes visible on hover)
- **Retry Failed**: Resources with "error" status can be re-scraped by deleting and re-adding

### 4. Chatting with Your Knowledge Base
1. Type your question in the chat interface
2. The AI will search your knowledge base for relevant information (top-5 documents)
3. Responses include source citations when relevant content is found
4. Chat history is automatically saved and accessible from the "Recent Conversations" accordion

### 5. Managing Chat Sessions
- **Create**: Click "New Chat" button
- **View**: Click on any previous chat in the "Recent Conversations" accordion
- **Delete**: Hover over a chat and click the trash icon

## ğŸ› ï¸ Development

### Project Structure
```
knowledge-base-agent/
â”œâ”€â”€ knowledge-base-agent-backend/     # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/endpoints/           # API route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py             # Chat endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ scrape.py           # Scraping (HTML + PDF URLs)
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py           # Document upload
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.py          # Resource management (+ DELETE)
â”‚   â”‚   â”‚   â””â”€â”€ query.py            # Direct queries
â”‚   â”‚   â”œâ”€â”€ core/                    # Configuration and database
â”‚   â”‚   â”œâ”€â”€ models/                  # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas/                 # Pydantic schemas
â”‚   â”‚   â””â”€â”€ services/                # Business logic services
â”‚   â”‚       â”œâ”€â”€ scraper.py          # Web + PDF scraping
â”‚   â”‚       â”œâ”€â”€ document_processor.py
â”‚   â”‚       â”œâ”€â”€ vector_store.py     # ChromaDB integration
â”‚   â”‚       â””â”€â”€ llm.py              # Ollama integration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ knowledge-base-agent-frontend/    # Next.js 15 frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                     # Next.js app router
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx            # Main layout with accordions
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Accordion.tsx   # Collapsible component
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ResourcesList.tsx # With delete functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ScrapeForm.tsx
â”‚   â”‚   â”‚   â””â”€â”€ documents/
â”‚   â”‚   â”‚       â””â”€â”€ DocumentUpload.tsx
â”‚   â”‚   â”œâ”€â”€ lib/                     # Utilities and API client
â”‚   â”‚   â”œâ”€â”€ stores/                  # Zustand state management
â”‚   â”‚   â”œâ”€â”€ styles/                  # Sass/SCSS files
â”‚   â”‚   â”‚   â”œâ”€â”€ accordion.scss      # Accordion & resource styles
â”‚   â”‚   â”‚   â”œâ”€â”€ sidebar.scss
â”‚   â”‚   â”‚   â””â”€â”€ layout.scss
â”‚   â”‚   â””â”€â”€ types/                   # TypeScript definitions
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ docs/                            # Documentation
â”‚   â”œâ”€â”€ analysis_summary.md
â”‚   â”œâ”€â”€ architecture_analysis.md
â”‚   â”œâ”€â”€ business_logic_flow.md
â”‚   â”œâ”€â”€ technical_integration.md
â”‚   â””â”€â”€ PRESENTATION_SLIDES.md
â”œâ”€â”€ install.sh                       # Automated installation
â”œâ”€â”€ start-dev.sh                     # Development startup script
â”œâ”€â”€ README.md
â””â”€â”€ STATUS.md
```

### Key Technologies
- **Backend**: FastAPI, SQLAlchemy, PostgreSQL, ChromaDB, Playwright, BeautifulSoup, PyPDF2
- **Frontend**: Next.js 15, React 19, TypeScript, Sass/SCSS, Zustand
- **AI/ML**: Ollama (llama3.1:8b, nomic-embed-text), ChromaDB
- **Database**: PostgreSQL 15 with JSONB support

## ğŸ” Troubleshooting

### Common Issues

1. **PostgreSQL Connection Error**
   ```bash
   brew services restart postgresql@15
   ```

2. **Ollama Models Not Found**
   ```bash
   ollama pull llama3.1:8b
   ollama pull nomic-embed-text
   ```

3. **Frontend Build Issues**
   ```bash
   cd knowledge-base-agent-frontend
   rm -rf node_modules package-lock.json
   npm install
   ```

4. **Backend Import Errors**
   ```bash
   cd knowledge-base-agent-backend
   source venv/bin/activate
   export PYTHONPATH="${PYTHONPATH}:$(pwd)"
   ```

5. **ChromaDB Schema Errors**
   ```bash
   # Delete and recreate vector database
   rm -rf knowledge-base-agent-backend/data/chroma_db
   mkdir -p knowledge-base-agent-backend/data/chroma_db
   ```

### Logs and Debugging
- Backend logs: Check terminal running uvicorn
- Frontend logs: Check browser console and terminal running npm
- Database logs: Check PostgreSQL logs via `brew services`
- API Testing: Use http://localhost:8000/docs (Swagger UI)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Built with FastAPI, Next.js 15, and Ollama
- Uses ChromaDB for vector storage
- Powered by local AI models for complete privacy
- PDF processing with PyPDF2
- Web scraping with Playwright and BeautifulSoup

## ğŸ“ Recent Updates

- âœ… Added accordion-based navigation for clean UI
- âœ… Implemented resource deletion with CASCADE
- âœ… Added PDF URL scraping support
- âœ… Widened sidebar for better content display (28-36rem)
- âœ… Added hover-based delete buttons with smooth transitions
- âœ… Improved text overflow handling with ellipsis
- âœ… Added retry logic for failed scrapes
- âœ… Enhanced resource cards with distinct visual separation
