# Knowledge Base Agent

A sophisticated personal knowledge management system that intelligently scrapes, processes, and synthesizes information from web sources and documents. The system provides a powerful chat-based interface with accordion navigation, full resource management, and chat history, querying a curated personal knowledge base using locally-run AI models.

## üèóÔ∏è Architecture

**Two-Part Solution:**
1. **Backend (Python/FastAPI)**: Web scraping (including PDF URLs), document processing, vector database, RAG implementation, and PostgreSQL for data persistence
2. **Frontend (Next.js 15)**: Modern accordion-based UI with clean navigation, chat interface, and resource management

## üöÄ Features

- ‚úÖ **Accordion-Based UI**: Clean, collapsible navigation reducing visual clutter
- ‚úÖ **PostgreSQL Integration**: Robust data storage with JSONB support for metadata
- ‚úÖ **Real-time Chat**: Instant messaging with AI responses and source attribution
- ‚úÖ **Source Attribution**: AI responses include relevant source citations with URLs
- ‚úÖ **Resource Management**: Full CRUD operations - view, add, and delete knowledge sources
- ‚úÖ **Mobile Responsive**: Works on desktop and mobile devices with wider sidebar (28-36rem)
- ‚úÖ **Local AI**: Complete privacy with local LLM processing (Ollama llama3.1:8b)
- ‚úÖ **Vector Search**: Semantic search with ChromaDB and nomic-embed-text
- ‚úÖ **Chat Management**: Create, view, and delete chat sessions with CASCADE
- ‚úÖ **Web Scraping**: Intelligent content extraction with Playwright + BeautifulSoup
- ‚úÖ **PDF URL Scraping**: Direct scraping of PDF files from web URLs using PyPDF2
- ‚úÖ **Document Upload**: Upload and process PDF, TXT, and EPUB files (up to 100MB)
- ‚úÖ **Retry Logic**: Automatic retry for failed scrapes
- ‚úÖ **Hover Interactions**: Subtle delete buttons with smooth opacity transitions

## ‚ö° Quick Start

**Run everything with one command:**

```bash
# Start all services (PostgreSQL, Ollama, Backend, Frontend)
make dev
```

**Or use the startup script directly:**

```bash
./start-dev.sh
```

**Available commands:**
- `make dev` - Start all services
- `make stop` - Stop all services
- `make setup` - Initial setup and installation
- `make clean` - Clean up generated files

The script automatically:
- ‚úÖ Checks system requirements
- ‚úÖ Starts PostgreSQL and creates database
- ‚úÖ Starts Ollama and downloads AI models
- ‚úÖ Installs dependencies if needed
- ‚úÖ Starts backend and frontend servers
- ‚úÖ Provides helpful status messages

**Access points:**
- üåê **Frontend**: http://localhost:3000
- üîß **Backend API**: http://localhost:8000
- üìö **API Docs**: http://localhost:8000/docs

Press `Ctrl+C` to stop all services.

## üìã Prerequisites

- **macOS** (optimized for M4 Pro)
- **Python 3.11**
- **Node.js 18+**
- **PostgreSQL 15**
- **Ollama** (for local AI models)

## üõ†Ô∏è Installation

### Option 1: Automated Installation (Recommended)

```bash
# Clone or navigate to the project directory
cd /Users/ekip.kalir/Projects/Personal/knowledge-base-agent

# Run the automated installation script
./install.sh
```

### Option 2: Manual Installation

#### 1. Install System Dependencies

```bash
# Install essential development tools
brew install curl wget tree htop git

# Install PostgreSQL 15
brew install postgresql@15
brew services start postgresql@15

# Install Python 3.11
brew install python@3.11

# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 2. Database Setup

```bash
# Create database and user
createdb knowledge_base_agent
psql knowledge_base_agent -c "CREATE USER kba_user WITH PASSWORD 'secure_password';"
psql knowledge_base_agent -c "GRANT ALL PRIVILEGES ON DATABASE knowledge_base_agent TO kba_user;"

# Create tables
psql knowledge_base_agent -c "
CREATE TABLE IF NOT EXISTS chat_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(255) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS chat_messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant')),
    sources JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS knowledge_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT UNIQUE NOT NULL,
    title VARCHAR(500),
    description TEXT,
    content TEXT,
    metadata JSONB,
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'error')),
    scraped_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_chat_messages_session_id ON chat_messages(session_id);
CREATE INDEX IF NOT EXISTS idx_chat_messages_created_at ON chat_messages(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_knowledge_sources_status ON knowledge_sources(status);
CREATE INDEX IF NOT EXISTS idx_knowledge_sources_url ON knowledge_sources(url);
"
```

#### 3. Backend Setup

```bash
cd knowledge-base-agent-backend

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
playwright install
```

#### 4. Frontend Setup

```bash
cd ../knowledge-base-agent-frontend

# Install dependencies
npm install
```

#### 5. Install AI Models

```bash
# Pull required Ollama models
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

## üöÄ Running the Application

### Option 1: Automated Startup (Recommended)

```bash
# Start all services with one command
./start-dev.sh
```

This will start:
- PostgreSQL (if not running)
- Ollama service
- Backend API server (port 8000)
- Frontend development server (port 3000)

### Option 2: Manual Startup

#### Terminal 1: Start Backend (Required)
```bash
# Quick start backend
./start-backend.sh

# OR manually:
cd knowledge-base-agent-backend
source venv/bin/activate
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Terminal 2: Start Frontend
```bash
cd knowledge-base-agent-frontend
npm run dev
```

#### Optional: Start Ollama (for AI features)
```bash
ollama serve
```

## üåê Access Points

- **Frontend Application**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Database**: PostgreSQL on localhost:5432

## üìö API Endpoints

### Chat Endpoints
- `POST /api/v1/chat/sessions` - Create new chat session
- `GET /api/v1/chat/sessions` - Get recent chat sessions (last 15)
- `GET /api/v1/chat/sessions/{id}/messages` - Get messages for a session
- `POST /api/v1/chat/sessions/{id}/messages` - Send message and get AI response with RAG
- `DELETE /api/v1/chat/sessions/{id}` - Delete chat session (CASCADE deletes messages)

### Knowledge Base Endpoints
- `POST /api/v1/scrape` - Scrape web content (including PDF URLs) and add to knowledge base
- `POST /api/v1/upload` - Upload documents (PDF, TXT, EPUB) to knowledge base
- `GET /api/v1/sources` - Get all knowledge sources with status
- `DELETE /api/v1/sources/{id}` - Delete knowledge source (CASCADE)
- `POST /api/v1/query` - Query knowledge base directly

## üîß Configuration

### Backend Configuration (.env)
```env
# Application Settings
APP_NAME="Knowledge Base Agent Backend"
APP_VERSION="0.1.0"
DEBUG=true

# Database Settings
DATABASE_URL="postgresql://kba_user:secure_password@localhost:5432/knowledge_base_agent"

# Vector Database
VECTOR_DB_TYPE="chromadb"
CHROMA_DB_PATH="./data/chroma_db"

# Ollama Settings
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_CHAT_MODEL="llama3.1:8b"
OLLAMA_EMBED_MODEL="nomic-embed-text"

# API Settings
API_V1_STR="/api/v1"
CORS_ORIGINS=["http://localhost:3000"]
```

### Frontend Configuration (.env.local)
```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_APP_NAME="Knowledge Base Agent"
NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws
```

## üìñ Usage Guide

### 1. First Time Setup
1. Start the application using `./start-dev.sh`
2. Navigate to http://localhost:3000
3. Click "New Chat" to start your first conversation

### 2. Adding Knowledge Sources

#### Using the Accordion UI
1. Open the "Add Web Source" accordion
2. Enter a URL (supports both HTML pages and PDF URLs like `https://arxiv.org/pdf/2510.06255`)
3. Click "Scrape URL"

#### Web Scraping (API)
```bash
# Scrape HTML page
curl -X POST "http://localhost:8000/api/v1/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/article"}'

# Scrape PDF URL directly
curl -X POST "http://localhost:8000/api/v1/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://arxiv.org/pdf/2510.06255"}'
```

#### Document Upload
```bash
# Upload a document via curl
curl -X POST "http://localhost:8000/api/v1/upload" \
  -F "file=@/path/to/your/document.pdf"

# Supported formats: PDF, TXT, EPUB
# Maximum file size: 100MB
```

#### Using the Web Interface
1. Navigate to http://localhost:3000
2. Open the "Upload Documents" accordion
3. Drag and drop files or click "Choose File" to browse
4. Supported formats: PDF, TXT, EPUB files

### 3. Managing Resources
- **View**: Open "Your Resources" accordion to see all indexed content
- **Delete**: Hover over any resource card and click the trash icon (becomes visible on hover)
- **Retry Failed**: Resources with "error" status can be re-scraped by deleting and re-adding

### 4. Chatting with Your Knowledge Base
1. Type your question in the chat interface
2. The AI will search your knowledge base for relevant information (top-5 documents)
3. Responses include source citations when relevant content is found
4. Chat history is automatically saved and accessible from the "Recent Conversations" accordion

### 5. Managing Chat Sessions
- **Create**: Click "New Chat" button
- **View**: Click on any previous chat in the "Recent Conversations" accordion
- **Delete**: Hover over a chat and click the trash icon

## üõ†Ô∏è Development

### Project Structure
```
knowledge-base-agent/
‚îú‚îÄ‚îÄ knowledge-base-agent-backend/     # Python FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/endpoints/           # API route handlers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py             # Chat endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrape.py           # Scraping (HTML + PDF URLs)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload.py           # Document upload
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources.py          # Resource management (+ DELETE)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query.py            # Direct queries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Configuration and database
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                  # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/                 # Pydantic schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/                # Business logic services
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ scraper.py          # Web + PDF scraping
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ document_processor.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vector_store.py     # ChromaDB integration
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ llm.py              # Ollama integration
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ knowledge-base-agent-frontend/    # Next.js 15 frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/                     # Next.js app router
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Main layout with accordions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Accordion.tsx   # Collapsible component
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ChatInterface.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ResourcesList.tsx # With delete functionality
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraping/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ScrapeForm.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ documents/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ DocumentUpload.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/                     # Utilities and API client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stores/                  # Zustand state management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ styles/                  # Sass/SCSS files
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accordion.scss      # Accordion & resource styles
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sidebar.scss
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout.scss
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/                   # TypeScript definitions
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ docs/                            # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ analysis_summary.md
‚îÇ   ‚îú‚îÄ‚îÄ architecture_analysis.md
‚îÇ   ‚îú‚îÄ‚îÄ business_logic_flow.md
‚îÇ   ‚îú‚îÄ‚îÄ technical_integration.md
‚îÇ   ‚îî‚îÄ‚îÄ PRESENTATION_SLIDES.md
‚îú‚îÄ‚îÄ install.sh                       # Automated installation
‚îú‚îÄ‚îÄ start-dev.sh                     # Development startup script
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ STATUS.md
```

### Key Technologies
- **Backend**: FastAPI, SQLAlchemy, PostgreSQL, ChromaDB, Playwright, BeautifulSoup, PyPDF2
- **Frontend**: Next.js 15, React 19, TypeScript, Sass/SCSS, Zustand
- **AI/ML**: Ollama (llama3.1:8b, nomic-embed-text), ChromaDB
- **Database**: PostgreSQL 15 with JSONB support

## üîç Troubleshooting

### Common Issues

1. **PostgreSQL Connection Error**
   ```bash
   brew services restart postgresql@15
   ```

2. **Ollama Models Not Found**
   ```bash
   ollama pull llama3.1:8b
   ollama pull nomic-embed-text
   ```

3. **Frontend Build Issues**
   ```bash
   cd knowledge-base-agent-frontend
   rm -rf node_modules package-lock.json
   npm install
   ```

4. **Backend Import Errors**
   ```bash
   cd knowledge-base-agent-backend
   source venv/bin/activate
   export PYTHONPATH="${PYTHONPATH}:$(pwd)"
   ```

5. **ChromaDB Schema Errors**
   ```bash
   # Delete and recreate vector database
   rm -rf knowledge-base-agent-backend/data/chroma_db
   mkdir -p knowledge-base-agent-backend/data/chroma_db
   ```

### Logs and Debugging
- Backend logs: Check terminal running uvicorn
- Frontend logs: Check browser console and terminal running npm
- Database logs: Check PostgreSQL logs via `brew services`
- API Testing: Use http://localhost:8000/docs (Swagger UI)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License.

## üôè Acknowledgments

- Built with FastAPI, Next.js 15, and Ollama
- Uses ChromaDB for vector storage
- Powered by local AI models for complete privacy
- PDF processing with PyPDF2
- Web scraping with Playwright and BeautifulSoup

## üìù Recent Updates

- ‚úÖ Added accordion-based navigation for clean UI
- ‚úÖ Implemented resource deletion with CASCADE
- ‚úÖ Added PDF URL scraping support
- ‚úÖ Widened sidebar for better content display (28-36rem)
- ‚úÖ Added hover-based delete buttons with smooth transitions
- ‚úÖ Improved text overflow handling with ellipsis
- ‚úÖ Added retry logic for failed scrapes
- ‚úÖ Enhanced resource cards with distinct visual separation
