# Knowledge Base Agent - Business Logic Flow Analysis

## ðŸ“Š Detailed Business Logic Flows

### 1. Complete Chat Message Processing Pipeline

```mermaid
graph TD
    A[User Types Message] --> B[Frontend Validation]
    B --> C[Send to Backend API]
    C --> D[Save User Message to SQLite]
    D --> E[Generate Query Embedding]
    E --> F[Search Vector Store ChromaDB]
    F --> G[Retrieve Top 5 Relevant Documents]
    G --> H[Assemble Context with Sources]
    H --> I[Create Structured Prompt]
    I --> J[Send to Ollama LLM]
    J --> K[Generate AI Response]
    K --> L[Save AI Message with Sources]
    L --> M[Update Session Timestamp]
    M --> N[Return Response to Frontend]
    N --> O[Display in Chat UI]
    
    style A fill:#e1f5fe
    style J fill:#fff3e0
    style F fill:#f3e5f5
    style D fill:#e8f5e8
```

### 2. Enhanced Knowledge Ingestion Business Logic

```mermaid
graph TD
    A[Content Input] --> B{Input Type?}
    B -->|Web URL| C[Playwright Browser Launch]
    B -->|PDF File| D[Enhanced PDF Processing]
    B -->|TXT File| E[Improved Text Reading]
    B -->|EPUB File| F[Enhanced EPUB Processing]
    
    C --> G[Navigate to URL]
    G --> H[Wait for Network Idle]
    H --> I[Extract Page Content]
    I --> J[BeautifulSoup Parsing]
    
    D --> D1{PyPDF2 Available?}
    D1 -->|Yes| D2[PyPDF2 Extraction]
    D1 -->|No| D3[Fallback Text Extraction]
    
    F --> F1{ebooklib Available?}
    F1 -->|Yes| F2[ebooklib Processing]
    F1 -->|No| F3[ZIP-based Fallback]
    
    E --> E1[Multiple Encoding Support]
    
    D2 --> N[Clean Text Content]
    D3 --> N
    E1 --> N
    F2 --> N
    F3 --> N
    J --> N
    
    N --> O[Text Chunking 1000 chars]
    O --> P[200 char Overlap Processing]
    P --> Q[Generate Embeddings]
    Q --> R[Store in ChromaDB]
    R --> S[Save Metadata to SQLite]
    S --> T[Update Source Status]
    
    style A fill:#e1f5fe
    style Q fill:#fff3e0
    style R fill:#f3e5f5
    style S fill:#e8f5e8
```

### 3. RAG (Retrieval-Augmented Generation) Process

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant V as Vector Store
    participant L as LLM (Ollama)
    participant D as SQLite DB
    
    U->>F: Send Message
    F->>B: POST /chat/sessions/{id}/messages
    B->>D: Save User Message
    B->>V: Generate Query Embedding
    V->>V: Cosine Similarity Search
    V->>B: Return Top 5 Documents
    B->>B: Assemble Context + Prompt
    B->>L: Generate Response
    L->>B: AI Response
    B->>D: Save AI Message + Sources
    B->>D: Update Session Timestamp
    B->>F: Return Complete Response
    F->>U: Display Chat + Sources
```

### 4. Document Processing State Machine

```mermaid
stateDiagram-v2
    [*] --> Pending: Document Uploaded/URL Submitted
    Pending --> Processing: Start Processing
    Processing --> Extracting: Content Extraction
    Extracting --> Chunking: Text Chunking
    Chunking --> Embedding: Generate Embeddings
    Embedding --> Storing: Store in Vector DB
    Storing --> Completed: Success
    
    Processing --> Error: Extraction Failed
    Extracting --> Error: Parsing Failed
    Chunking --> Error: Chunking Failed
    Embedding --> Error: Embedding Failed
    Storing --> Error: Storage Failed
    
    Error --> Pending: Retry
    Completed --> [*]
```

### 5. Chat Session Lifecycle

```mermaid
graph LR
    A[New Chat Button] --> B[Create Session]
    B --> C[Generate String UUID]
    C --> D[Save to SQLite]
    D --> E[Active Chat Session]
    
    E --> F[Send Messages]
    F --> G[AI Responses]
    G --> H[Update Session Timestamp]
    H --> E
    
    E --> I[Delete Session]
    I --> J[Cascade Delete Messages]
    J --> K[Session Removed]
    
    E --> L[Session Idle]
    L --> M[Show in Sidebar]
    M --> N[Click to Resume]
    N --> E
    
    style B fill:#e8f5e8
    style I fill:#ffebee
```

## ðŸ”§ Service Integration Patterns

### 1. Vector Store Integration Pattern
```python
# Business Logic Flow in VectorStore Service
async def add_document(content: str, metadata: Dict) -> str:
    # 1. Text Chunking Strategy
    chunks = self._chunk_text(content, chunk_size=1000, overlap=200)
    
    # 2. Embedding Generation
    for chunk in chunks:
        embedding = self.embedder.encode(chunk).tolist()
        
        # 3. Metadata Enhancement
        chunk_metadata = {**metadata, "chunk_index": i, "total_chunks": len(chunks)}
        
        # 4. Vector Storage
        self.collection.add(ids=[doc_id], embeddings=[embedding], 
                           documents=[chunk], metadatas=[chunk_metadata])
```

### 2. Enhanced Document Processing Pattern
```python
# Enhanced Document Processing Logic
async def _process_pdf(self, file_content: bytes, filename: str) -> str:
    try:
        # Try to import PyPDF2 if available
        import PyPDF2
        import io
        
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        text_content = []
        
        for page in pdf_reader.pages:
            text_content.append(page.extract_text())
        
        extracted_text = "\n".join(text_content)
        return self._clean_text(extracted_text)
        
    except ImportError:
        # Fallback: extract any readable text from PDF bytes
        try:
            # Simple text extraction - look for readable content
            decoded_content = file_content.decode('latin-1', errors='ignore')
            # Basic text extraction from PDF structure
            import re
            text_patterns = re.findall(r'\(([^)]+)\)', decoded_content)
            text_content = ' '.join(text_patterns)
            
            if len(text_content.strip()) < 50:
                # If we can't extract much, provide a placeholder
                return f"PDF document: {filename}\n\nNote: PDF text extraction requires PyPDF2. Install with: pip install PyPDF2"
            
            return self._clean_text(text_content)
        except Exception:
            return f"PDF document: {filename}\n\nNote: Unable to extract text from this PDF. Please install PyPDF2 for better PDF support."
```

### 3. LLM Integration Pattern
```python
# RAG Context Assembly Business Logic
async def generate_response(query: str, context: List[Dict]) -> str:
    # 1. Context Preparation
    context_text = "\n\n".join([
        f"Source: {doc['metadata'].get('url', 'Unknown')}\n{doc['content']}"
        for doc in context
    ])
    
    # 2. Prompt Engineering
    prompt = f"""You are a helpful AI assistant with access to a personal knowledge base.
    Context: {context_text}
    Question: {query}
    Answer:"""
    
    # 3. LLM Generation with Parameters
    payload = {
        "model": self.chat_model,
        "prompt": prompt,
        "options": {"temperature": 0.7, "top_p": 0.9}
    }
```

### 4. Database Transaction Pattern
```python
# Chat Message Processing Transaction with SQLite
async def send_message(session_id: str, content: str, db: AsyncSession):
    # 1. Save User Message
    user_message = ChatMessage(session_id=session_id, content=content, role="user")
    db.add(user_message)
    
    # 2. Vector Search
    relevant_docs = await vector_store.search(content, n_results=5)
    
    # 3. Generate AI Response
    ai_response = await llm.generate_response(content, relevant_docs)
    
    # 4. Save AI Message with Sources
    ai_message = ChatMessage(session_id=session_id, content=ai_response, 
                           role="assistant", sources=sources)
    db.add(ai_message)
    
    # 5. Update Session Timestamp
    session.updated_at = func.now()
    
    # 6. Commit Transaction
    await db.commit()
```

## ðŸ“ˆ Performance & Scalability Considerations

### 1. Vector Search Performance
- **Chunk Size**: 1000 characters optimal for semantic coherence
- **Overlap**: 200 characters prevents context loss at boundaries
- **Top-K Retrieval**: Limited to 5 documents for response quality
- **Embedding Model**: nomic-embed-text (fast inference)

### 2. Database Optimization
- **SQLite Efficiency**: File-based storage with good read performance
- **JSON Support**: Flexible metadata storage
- **String UUIDs**: Simple string-based unique identifiers
- **Cascade Deletes**: Automatic cleanup of related records

### 3. Document Processing
- **Graceful Degradation**: Fallback mechanisms when optional dependencies are missing
- **Multiple Format Support**: Enhanced handling of PDF, EPUB, and TXT files
- **Error Handling**: Informative error messages and recovery options
- **File Size Limit**: Increased to 100MB for larger documents

### 4. LLM Processing
- **Local Processing**: Complete privacy, no external API calls
- **Model Size**: 8B parameters balanced for quality/speed
- **Context Window**: Large enough for multiple document chunks
- **Temperature**: 0.7 for balanced creativity/accuracy

This business logic architecture ensures efficient, scalable, and privacy-focused knowledge management with intelligent retrieval and generation capabilities.
